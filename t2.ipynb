{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wikipedia\n",
      "  Using cached wikipedia-1.4.0-py3-none-any.whl\n",
      "Collecting beautifulsoup4 (from wikipedia)\n",
      "  Using cached beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in ./myenv/lib/python3.11/site-packages (from wikipedia) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./myenv/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./myenv/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./myenv/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./myenv/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.7.4)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->wikipedia)\n",
      "  Using cached soupsieve-2.5-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4, wikipedia\n",
      "Successfully installed beautifulsoup4-4.12.3 soupsieve-2.5 wikipedia-1.4.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions:\n",
      "Can you tell me more about your experience with the University of California, Berkeley?\n",
      "Is the answer factual with respect to the CV? False\n",
      "Is the answer general knowledge? True\n",
      "\n",
      "Can you tell me more about your experience with SQL?\n",
      "Is the answer factual with respect to the CV? False\n",
      "Is the answer general knowledge? True\n",
      "\n",
      "Can you tell me more about your experience with 5 years?\n",
      "Is the answer factual with respect to the CV? True\n",
      "Is the answer general knowledge? True\n",
      "\n",
      "Can you tell me more about your experience with Python?\n",
      "Is the answer factual with respect to the CV? False\n",
      "Is the answer general knowledge? False\n",
      "\n",
      "How have you applied your software expertise in your previous roles?\n",
      "Is the answer factual with respect to the CV? False\n",
      "Is the answer general knowledge? False\n",
      "\n",
      "How have you applied your engineer expertise in your previous roles?\n",
      "Is the answer factual with respect to the CV? True\n",
      "Is the answer general knowledge? False\n",
      "\n",
      "How have you applied your years expertise in your previous roles?\n",
      "Is the answer factual with respect to the CV? False\n",
      "Is the answer general knowledge? True\n",
      "\n",
      "How have you applied your experience expertise in your previous roles?\n"
     ]
    },
    {
     "ename": "WikipediaException",
     "evalue": "An unknown error occured: \"The \"srsearch\" parameter must be set.\". Please report it on GitHub!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mWikipediaException\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 132\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28mprint\u001b[39m(question)\n\u001b[1;32m    131\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour answer: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 132\u001b[0m is_factual_cv, is_general_knowledge \u001b[38;5;241m=\u001b[39m \u001b[43manalyze_answers\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43manswer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIs the answer factual with respect to the CV? \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mis_factual_cv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIs the answer general knowledge? \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mis_general_knowledge\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 110\u001b[0m, in \u001b[0;36manalyze_answers\u001b[0;34m(sentences, answers)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m answer \u001b[38;5;129;01min\u001b[39;00m answers:\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 110\u001b[0m         page \u001b[38;5;241m=\u001b[39m \u001b[43mwikipedia\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpage\u001b[49m\u001b[43m(\u001b[49m\u001b[43manswer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m         is_general_knowledge \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    112\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/T2/myenv/lib/python3.11/site-packages/wikipedia/wikipedia.py:270\u001b[0m, in \u001b[0;36mpage\u001b[0;34m(title, pageid, auto_suggest, redirect, preload)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m title \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    269\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m auto_suggest:\n\u001b[0;32m--> 270\u001b[0m     results, suggestion \u001b[38;5;241m=\u001b[39m \u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresults\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuggestion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    272\u001b[0m       title \u001b[38;5;241m=\u001b[39m suggestion \u001b[38;5;129;01mor\u001b[39;00m results[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/T2/myenv/lib/python3.11/site-packages/wikipedia/util.py:28\u001b[0m, in \u001b[0;36mcache.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache[key]\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 28\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache[key] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/Desktop/T2/myenv/lib/python3.11/site-packages/wikipedia/wikipedia.py:109\u001b[0m, in \u001b[0;36msearch\u001b[0;34m(query, results, suggestion)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPTimeoutError(query)\n\u001b[1;32m    108\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 109\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m WikipediaException(raw_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minfo\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    111\u001b[0m search_results \u001b[38;5;241m=\u001b[39m (d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m raw_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msearch\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m suggestion:\n",
      "\u001b[0;31mWikipediaException\u001b[0m: An unknown error occured: \"The \"srsearch\" parameter must be set.\". Please report it on GitHub!"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "from spacy.tokens import Span\n",
    "import re\n",
    "import random\n",
    "import wikipedia\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_sentences(cv_file):\n",
    "    # Read the CV file\n",
    "    with open(cv_file, 'r') as file:\n",
    "        cv_text = file.read()\n",
    "\n",
    "    # Split the CV text into sentences\n",
    "    sentences = re.split(r'[.!?]+', cv_text)\n",
    "    return sentences\n",
    "\n",
    "def extract_entities(sentences):\n",
    "    # Extract named entities and their types from the sentences\n",
    "    all_entities = []\n",
    "    all_entity_labels = []\n",
    "    for sentence in sentences:\n",
    "        doc = nlp(sentence)\n",
    "        entities = [ent for ent in doc.ents]\n",
    "        all_entities.extend(entities)\n",
    "        all_entity_labels.extend([ent.label_ for ent in entities])\n",
    "\n",
    "    # Count the most common entity types\n",
    "    entity_counts = Counter(all_entity_labels)\n",
    "    top_entity_types = [ent_type for ent_type, count in entity_counts.most_common(3)]\n",
    "\n",
    "    return all_entities, top_entity_types\n",
    "\n",
    "def identify_skills(sentences):\n",
    "    # Extract skills from the sentences\n",
    "    skills = []\n",
    "    for sentence in sentences:\n",
    "        doc = nlp(sentence)\n",
    "        skills.extend([token.text for token in doc if not token.is_stop and not token.is_punct and token.pos_ == \"NOUN\"])\n",
    "\n",
    "    # Count the most common skills\n",
    "    skill_counts = Counter(skills)\n",
    "    top_skills = [skill for skill, count in skill_counts.most_common(5)]\n",
    "\n",
    "    return top_skills\n",
    "\n",
    "def summarize_cv(sentences):\n",
    "    # Summarize the CV content\n",
    "    summary = \"\"\n",
    "    for sentence in sentences:\n",
    "        doc = nlp(sentence)\n",
    "        summary += \" \" + sentence\n",
    "\n",
    "    return summary\n",
    "\n",
    "def generate_questions(entities, top_entity_types, top_skills, summary):\n",
    "    # Generate questions based on the extracted information\n",
    "    questions = []\n",
    "    follow_up_questions = []\n",
    "\n",
    "    # Entity-based questions\n",
    "    for ent_type in top_entity_types:\n",
    "        ent_questions = [f\"Can you tell me more about your experience with {ent.text}?\" for ent in entities if ent.label_ == ent_type]\n",
    "        questions.extend(ent_questions)\n",
    "\n",
    "    # Skill-based questions\n",
    "    for skill in top_skills:\n",
    "        skill_question = f\"How have you applied your {skill} expertise in your previous roles?\"\n",
    "        questions.append(skill_question)\n",
    "\n",
    "    # Summarization-based questions\n",
    "    summary_question = f\"I noticed the summary mentioned {summary}. Can you elaborate on that and your specific contributions?\"\n",
    "    questions.append(summary_question)\n",
    "\n",
    "    # Follow-up questions\n",
    "    for question in questions:\n",
    "        follow_up_question = f\"What was your role in the {question.split('?')[0].split(' ')[-1]}?\"\n",
    "        follow_up_questions.append(follow_up_question)\n",
    "\n",
    "    return questions, follow_up_questions\n",
    "\n",
    "def analyze_answers(sentences, answers):\n",
    "    # Extract relevant information from the CV\n",
    "    skills = identify_skills(sentences)\n",
    "    education = []\n",
    "    experience = []\n",
    "    for sentence in sentences:\n",
    "        doc = nlp(sentence)\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == \"PERSON\":\n",
    "                education.append(ent.text)\n",
    "            elif ent.label_ in [\"ORG\", \"GPE\", \"WORK_OF_ART\"]:\n",
    "                experience.append(ent.text)\n",
    "\n",
    "    # Check if the answers are factual with respect to the CV\n",
    "    is_factual_cv = True\n",
    "    for answer in answers:\n",
    "        if not any(skill in answer.lower() for skill in skills) and \\\n",
    "           not any(edu in answer.lower() for edu in education) and \\\n",
    "           not any(exp in answer.lower() for exp in experience):\n",
    "            is_factual_cv = False\n",
    "            break\n",
    "\n",
    "    # Check if the answers are general knowledge\n",
    "    is_general_knowledge = False\n",
    "    for answer in answers:\n",
    "        try:\n",
    "            page = wikipedia.page(answer)\n",
    "            is_general_knowledge = True\n",
    "            break\n",
    "        except wikipedia.exceptions.PageError:\n",
    "            pass\n",
    "\n",
    "    return is_factual_cv, is_general_knowledge\n",
    "\n",
    "# Example usage\n",
    "cv_file = \"cv.txt\"\n",
    "sentences = extract_sentences(cv_file)\n",
    "\n",
    "entities, top_entity_types = extract_entities(sentences)\n",
    "top_skills = identify_skills(sentences)\n",
    "summary = summarize_cv(sentences)\n",
    "\n",
    "questions, follow_up_questions = generate_questions(entities, top_entity_types, top_skills, summary)\n",
    "\n",
    "print(\"Questions:\")\n",
    "for question in questions:\n",
    "    print(question)\n",
    "    answer = input(f\"Your answer: \")\n",
    "    is_factual_cv, is_general_knowledge = analyze_answers(sentences, [answer])\n",
    "    print(f\"Is the answer factual with respect to the CV? {is_factual_cv}\")\n",
    "    print(f\"Is the answer general knowledge? {is_general_knowledge}\")\n",
    "    print()\n",
    "\n",
    "print(\"\\nFollow-up Questions:\")\n",
    "for question in follow_up_questions:\n",
    "    print(question)\n",
    "    answer = input(f\"Your answer: \")\n",
    "    print(answer)\n",
    "    is_factual_cv, is_general_knowledge = analyze_answers(sentences, [answer])\n",
    "    print(f\"Is the answer factual with respect to the CV? {is_factual_cv}\")\n",
    "    print(f\"Is the answer general knowledge? {is_general_knowledge}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "from spacy.tokens import Span\n",
    "import re\n",
    "import random\n",
    "import wikipedia\n",
    "from textblob import TextBlob\n",
    "import gtts\n",
    "from gtts import gTTS\n",
    "import os\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_sentences(cv_file):\n",
    "    # Read the CV file\n",
    "    with open(cv_file, 'r') as file:\n",
    "        cv_text = file.read()\n",
    "\n",
    "    # Split the CV text into sentences\n",
    "    sentences = re.split(r'[.!?]+', cv_text)\n",
    "    return sentences\n",
    "\n",
    "def extract_entities(sentences):\n",
    "    # Extract named entities and their types from the sentences\n",
    "    all_entities = []\n",
    "    all_entity_labels = []\n",
    "    for sentence in sentences:\n",
    "        doc = nlp(sentence)\n",
    "        entities = [ent for ent in doc.ents]\n",
    "        all_entities.extend(entities)\n",
    "        all_entity_labels.extend([ent.label_ for ent in entities])\n",
    "\n",
    "    # Count the most common entity types\n",
    "    entity_counts = Counter(all_entity_labels)\n",
    "    top_entity_types = [ent_type for ent_type, count in entity_counts.most_common(3)]\n",
    "\n",
    "    return all_entities, top_entity_types\n",
    "\n",
    "def identify_skills(sentences):\n",
    "    # Extract skills from the sentences\n",
    "    skills = []\n",
    "    for sentence in sentences:\n",
    "        doc = nlp(sentence)\n",
    "        skills.extend([token.text for token in doc if not token.is_stop and not token.is_punct and token.pos_ == \"NOUN\"])\n",
    "\n",
    "    # Count the most common skills\n",
    "    skill_counts = Counter(skills)\n",
    "    top_skills = [skill for skill, count in skill_counts.most_common(5)]\n",
    "\n",
    "    return top_skills\n",
    "\n",
    "def summarize_cv(sentences):\n",
    "    # Summarize the CV content\n",
    "    summary = \"\"\n",
    "    for sentence in sentences:\n",
    "        doc = nlp(sentence)\n",
    "        summary += \" \" + sentence\n",
    "\n",
    "    return summary\n",
    "\n",
    "def generate_questions(entities, top_entity_types, top_skills, summary):\n",
    "    # Generate questions based on the extracted information\n",
    "    questions = []\n",
    "    follow_up_questions = []\n",
    "\n",
    "    # Entity-based questions\n",
    "    for ent_type in top_entity_types:\n",
    "        ent_questions = [f\"Can you tell me more about your experience with {ent.text} during your time at {ent.root.head.text}?\" for ent in entities if ent.label_ == ent_type]\n",
    "        questions.extend(ent_questions)\n",
    "\n",
    "    # Skill-based questions\n",
    "    for skill in top_skills:\n",
    "        skill_question = f\"How have you utilized your {skill} skills in your previous roles?\"\n",
    "        questions.append(skill_question)\n",
    "\n",
    "    # Summarization-based questions\n",
    "    summary_question = f\"I noticed the summary mentioned {summary}. Can you elaborate on your specific contributions in that area?\"\n",
    "    questions.append(summary_question)\n",
    "\n",
    "    # Follow-up questions\n",
    "    follow_up_questions = [\n",
    "        \"Can you provide more details on your achievements in that role?\",\n",
    "        \"How did you overcome any challenges you faced while working on that project?\",\n",
    "        \"What was your specific contribution to the team's success in that endeavor?\"\n",
    "    ]\n",
    "\n",
    "    return questions, follow_up_questions\n",
    "\n",
    "def analyze_answers(sentences, answers):\n",
    "    # Analyze the answers for factual information, general knowledge, and harsh/non-serious responses\n",
    "    is_factual_cv = False\n",
    "    is_general_knowledge = False\n",
    "    is_harsh_or_non_serious = False\n",
    "\n",
    "    # Implement your answer analysis logic here\n",
    "    # ...\n",
    "\n",
    "    return is_factual_cv, is_general_knowledge, is_harsh_or_non_serious\n",
    "\n",
    "def conduct_interview(questions, follow_up_questions):\n",
    "    score = 100\n",
    "\n",
    "    print(\"Interview started. Please answer the following questions:\")\n",
    "\n",
    "    for i, question in enumerate(questions):\n",
    "        tts = gTTS(text=f\"Question {i+1}: {question}\", lang='en')\n",
    "        tts.save(f\"question{i+1}.mp3\")\n",
    "        os.system(f\"mpg123 question{i+1}.mp3\")\n",
    "        answer = input(f\"Answer {i+1}: \")\n",
    "        print(f\"You answered: {answer}\")\n",
    "        is_factual_cv, is_general_knowledge, is_harsh_or_non_serious = analyze_answers(sentences, [answer])\n",
    "        if is_harsh_or_non_serious:\n",
    "            score -= 10  # Decrease score by 10 for a harsh or non-serious response\n",
    "        if score <= 0:\n",
    "            tts = gTTS(text=\"Interview selection result: Not selected\", lang='en')\n",
    "            tts.save(\"result.mp3\")\n",
    "            os.system(\"mpg123 result.mp3\")\n",
    "            return\n",
    "        if is_factual_cv and is_general_knowledge:\n",
    "            tts = gTTS(text=\"Interview selection result: Selected\", lang='en')\n",
    "            tts.save(\"result.mp3\")\n",
    "            os.system(\"mpg123 result.mp3\")\n",
    "            break\n",
    "\n",
    "    print(\"\\nFollow-up Questions:\")\n",
    "    for i, follow_up_question in enumerate(follow_up_questions):\n",
    "        tts = gTTS(text=f\"Follow-up Question {i+1}: {follow_up_question}\", lang='en')\n",
    "        tts.save(f\"follow_up{i+1}.mp3\")\n",
    "        os.system(f\"mpg123 follow_up{i+1}.mp3\")\n",
    "        answer = input(f\"Answer {i+1}: \")\n",
    "        print(f\"You answered: {answer}\")\n",
    "        is_factual_cv, is_general_knowledge, is_harsh_or_non_serious = analyze_answers(sentences, [answer])\n",
    "        if is_harsh_or_non_serious:\n",
    "            score -= 10  # Decrease score by 10 for a harsh or non-serious response\n",
    "        if score <= 0:\n",
    "            tts = gTTS(text=\"Interview selection result: Not selected\", lang='en')\n",
    "            tts.save(\"result.mp3\")\n",
    "            os.system(\"mpg123 result.mp3\")\n",
    "            return\n",
    "        if is_factual_cv and is_general_knowledge:\n",
    "            tts = gTTS(text=\"Interview selection result: Selected\", lang='en')\n",
    "            tts.save(\"result.mp3\")\n",
    "            os.system(\"mpg123 result.mp3\")\n",
    "            break\n",
    "\n",
    "    if score > 0:\n",
    "        tts = gTTS(text=f\"Your final score is: {score}\", lang='en')\n",
    "        tts.save(\"result.mp3\")\n",
    "        os.system(\"mpg123 result.mp3\")\n",
    "    else:\n",
    "        tts = gTTS(text=\"Interview selection result: Not selected\", lang='en')\n",
    "        tts.save(\"result.mp3\")\n",
    "        os.system(\"mpg123 result.mp3\")\n",
    "\n",
    "# Example usage\n",
    "cv_file = \"cv.txt\"\n",
    "sentences = extract_sentences(cv_file)\n",
    "entities, top_entity_types = extract_entities(sentences)\n",
    "top_skills = identify_skills(sentences)\n",
    "summary = summarize_cv(sentences)\n",
    "questions, follow_up_questions = generate_questions(entities, top_entity_types, top_skills, summary)\n",
    "\n",
    "conduct_interview(questions, follow_up_questions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
